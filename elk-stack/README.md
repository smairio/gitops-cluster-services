# ELK Stack on Kubernetes (ECK)

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Kubernetes Cluster                              â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚    App Nodes (general)    â”‚   â”‚         ELK Nodes (elk)             â”‚ â”‚
â”‚  â”‚  label: node-type=general â”‚   â”‚  label: node-type=elk               â”‚ â”‚
â”‚  â”‚                           â”‚   â”‚  taint: dedicated=elk:NoSchedule    â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚                                     â”‚ â”‚
â”‚  â”‚  â”‚ AWX â”‚ â”‚Monitoring â”‚    â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚  â”‚    ECK Operator (Helm)       â”‚   â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â”‚  â”‚Keycloak  â”‚ â”‚ CNPG â”‚   â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â”‚ ES-0   â”‚ â”‚ ES-1   â”‚ â”‚ ES-2   â”‚  â”‚ â”‚
â”‚  â”‚                           â”‚   â”‚  â”‚ 50Gi   â”‚ â”‚ 50Gi   â”‚ â”‚ 50Gi   â”‚  â”‚ â”‚
â”‚  â”‚                           â”‚   â”‚  â”‚ PVCâ†’PV â”‚ â”‚ PVCâ†’PV â”‚ â”‚ PVCâ†’PV â”‚  â”‚ â”‚
â”‚  â”‚                           â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â”‚                           â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚ â”‚
â”‚  â”‚                           â”‚   â”‚  â”‚ Kibana   â”‚                      â”‚ â”‚
â”‚  â”‚                           â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚ â”‚
â”‚  â”‚  â”‚Filebeat  â”‚ DaemonSet   â”‚   â”‚  â”‚Filebeat  â”‚  â”‚Metricbeatâ”‚       â”‚ â”‚
â”‚  â”‚  â”‚Metricbeatâ”‚ (all nodes) â”‚   â”‚  â”‚Metricbeatâ”‚  â”‚          â”‚       â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                          â”‚
â”‚  Hetzner Block Storage (CSI)     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  StorageClass: hcloud-volumes   â”‚    â”‚
â”‚  â”‚ PV auto-provisioned by  â”‚â—„â”€â”€â”€â”€â”‚  binding: WaitForFirstConsumer  â”‚    â”‚
â”‚  â”‚ hcloud-csi driver when  â”‚     â”‚  expansion: true                â”‚    â”‚
â”‚  â”‚ PVC is bound to a pod   â”‚     â”‚  reclaim: Delete                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Components

| Component | Version | Namespace | Runs On | Purpose |
|-----------|---------|-----------|---------|---------|
| **ECK Operator** | 2.14.0 | elastic-system | ELK nodes | Manages all Elastic CRDs |
| **Elasticsearch** | 8.15.3 | elastic-system | ELK nodes | Log storage & search engine |
| **Kibana** | 8.15.3 | elastic-system | ELK nodes | Visualization & dashboards |
| **Filebeat** | 8.15.3 | elastic-system | **ALL nodes** | Log collection (DaemonSet) |
| **Metricbeat** | 8.15.3 | elastic-system | **ALL nodes** | System & K8s metrics (DaemonSet) |

## Folder Structure

```
eck-operator/              # ArgoCD app â€” installs the ECK CRD operator
â”œâ”€â”€ app.yaml               # ArgoCD Application (sync-wave: -1)
â””â”€â”€ values.yaml            # Helm values (nodeSelector, tolerations)

elk-stack/                 # ArgoCD app â€” deploys the actual ELK workloads
â”œâ”€â”€ app.yaml               # ArgoCD Application (sync-wave: 0)
â”œâ”€â”€ README.md              # This documentation
â””â”€â”€ manifests/
    â”œâ”€â”€ namespace.yaml     # elastic-system namespace
    â”œâ”€â”€ elasticsearch.yaml # 3-node ES cluster (50Gi each)
    â”œâ”€â”€ kibana.yaml        # Kibana instance
    â”œâ”€â”€ filebeat.yaml      # Log collection DaemonSet + RBAC
    â”œâ”€â”€ metricbeat.yaml    # Metrics collection DaemonSet + RBAC
    â””â”€â”€ ingress.yaml       # Kibana ingress (kibana.tests.software)
```

## Node Placement & Taints

All ELK components (except Filebeat/Metricbeat) are pinned to ELK-dedicated nodes:

```yaml
nodeSelector:
  node-type: elk
tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "elk"
    effect: "NoSchedule"
```

**Filebeat & Metricbeat** are DaemonSets that must collect from every node:

```yaml
tolerations:
  - operator: Exists      # Tolerates ALL taints â†’ runs on every node
```

## Sync Order (ArgoCD Sync Waves)

```
Wave -3: cloudnative-pg operator
Wave -2: monitoring, barman-cloud-plugin
Wave -1: eck-operator          â† ECK CRDs installed here
Wave  0: elk-stack, awx        â† ES/Kibana/Filebeat/Metricbeat created here
Wave  1: postgres-bootstrap
```

The ECK operator must be ready **before** Elasticsearch/Kibana CRDs are applied.

---

## Storage â€” Deep Dive

### How Volumes Are Created (Automatic Provisioning)

Elasticsearch uses a **StatefulSet** managed by the ECK operator. Storage is **fully automatic** â€” you never create PVs manually. Here is the exact flow:

```
1. You define a volumeClaimTemplate in elasticsearch.yaml:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ volumeClaimTemplates:                  â”‚
   â”‚   - metadata:                          â”‚
   â”‚       name: elasticsearch-data         â”‚
   â”‚     spec:                              â”‚
   â”‚       storageClassName: hcloud-volumes  â”‚
   â”‚       resources:                       â”‚
   â”‚         requests:                      â”‚
   â”‚           storage: 50Gi               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
2. ECK Operator creates a StatefulSet with 3 replicas (ES nodes)
                    â”‚
                    â–¼
3. StatefulSet creates PVCs (one per replica):
   â€¢ elasticsearch-data-elk-es-default-0  (50Gi)
   â€¢ elasticsearch-data-elk-es-default-1  (50Gi)
   â€¢ elasticsearch-data-elk-es-default-2  (50Gi)
                    â”‚
                    â–¼
4. Pod is scheduled to an ELK node (nodeSelector: node-type=elk)
                    â”‚
                    â–¼
5. WaitForFirstConsumer: PV is NOT created until a pod is bound
   â†’ CSI driver sees which zone/node the pod lands on
   â†’ Creates a Hetzner Block Volume IN THAT SAME ZONE
   â†’ Attaches it to the node, formats it, mounts it
                    â”‚
                    â–¼
6. PVC â†’ PV binding is complete. ES node starts writing data.
```

**You do nothing.** The CSI driver handles volume creation, formatting, attaching, and mounting automatically.

### StorageClass Configuration

The `hcloud-volumes` StorageClass is deployed by the CSI driver addon:

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: hcloud-volumes
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: csi.hetzner.cloud
volumeBindingMode: WaitForFirstConsumer   # â† Key: zone-aware provisioning
allowVolumeExpansion: true                # â† Key: online resize supported
reclaimPolicy: Delete                     # â† Key: PV deleted when PVC is removed
```

| Setting | Value | Why |
|---------|-------|-----|
| `volumeBindingMode` | `WaitForFirstConsumer` | PV is created in the same zone as the pod. Prevents cross-zone mounting failures. |
| `allowVolumeExpansion` | `true` | You can increase PVC size without downtime. Hetzner grows the block device online. |
| `reclaimPolicy` | `Delete` | When a PVC is deleted, the underlying Hetzner Volume is **destroyed**. This is the default and **correct for most cases** â€” see retention section below. |

### Volume Lifecycle â€” What Happens When

| Event | PVC | PV | Hetzner Volume | Data |
|-------|-----|-----|----------------|------|
| ES pod created | Created by StatefulSet | Auto-provisioned by CSI | Created in Hetzner Cloud | Empty |
| ES pod restarted | Unchanged | Unchanged | Unchanged | **Preserved** âœ… |
| ES pod rescheduled to same node | Unchanged | Re-mounted | Re-attached | **Preserved** âœ… |
| ES pod deleted (scale down) | **Still exists** | **Still bound** | **Still exists** | **Preserved** âœ… |
| ES pod re-created (scale up) | Re-used | Re-mounted | Re-attached | **Preserved** âœ… |
| PVC manually deleted | Deleted | Deleted (reclaimPolicy=Delete) | **DESTROYED** âŒ | **LOST** âŒ |
| Namespace deleted | All PVCs deleted | All PVs deleted | **ALL DESTROYED** âŒ | **ALL LOST** âŒ |
| `kubectl delete elasticsearch elk` | StatefulSet deleted, PVCs **kept** | Still bound | Still exists | **Preserved** âœ… |

> âš ï¸ **Critical:** StatefulSet PVCs are NOT automatically deleted when you delete the Elasticsearch CR or scale down. This is a Kubernetes safety feature. Data is preserved until you explicitly delete the PVCs.

### Reclaim Policy â€” Delete vs Retain

| Policy | Behavior | Use Case |
|--------|----------|----------|
| **Delete** (current) | PV + Hetzner Volume destroyed when PVC is deleted | Dev/staging, or production with snapshot backups |
| **Retain** | PV becomes `Released`, Hetzner Volume preserved even after PVC deletion | Extra safety â€” but you must manually clean up orphaned volumes |

#### How to Switch to Retain (if needed)

**Option A â€” Patch existing PVs** (per-volume, does NOT affect future PVs):

```bash
# List ES volumes
kubectl get pv | grep elasticsearch-data

# Patch each PV to Retain
kubectl patch pv <pv-name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'
```

**Option B â€” Create a separate StorageClass** (for all future ELK volumes):

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hcloud-volumes-retain
provisioner: csi.hetzner.cloud
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Retain                     # â† PV kept after PVC deletion
```

Then change `elasticsearch.yaml` to use `storageClassName: hcloud-volumes-retain`.

> ğŸ’¡ **Professional recommendation:** Keep `Delete` policy and use **Elasticsearch Snapshots** for backups. Retain creates orphaned volumes that cost money and require manual cleanup.

### Volume Expansion â€” Growing Storage Online

Since `allowVolumeExpansion: true` is set, you can grow volumes **without any downtime**:

1. Edit `elasticsearch.yaml` â†’ change `storage: 50Gi` to `storage: 100Gi`
2. Commit & push â†’ ArgoCD syncs the change
3. ECK operator performs a **rolling update** â€” one ES node at a time
4. For each node:
   - Pod is stopped
   - CSI driver calls Hetzner API to resize the block volume
   - Filesystem is expanded (online, ext4/xfs)
   - Pod restarts with the larger volume
5. Zero data loss â€” ES rebalances shards automatically

> âš ï¸ **You can only increase storage size, never decrease it.**

### Sizing Guidelines

| Cluster Size | ES Nodes | Storage per Node | Total | Use Case |
|:-------------|:---------|:-----------------|:------|:---------|
| Small/Dev    | 3        | 20Gi             | 60Gi  | Development, low log volume |
| **Default**  | **3**    | **50Gi**         | **150Gi** | **Production, moderate logs** |
| Large        | 3â€“5      | 100Gi            | 300â€“500Gi | High log volume, long retention |

**Estimating storage needs:**
- Average log line â‰ˆ 500 bytes (after ES indexing overhead â‰ˆ 1â€“1.5KB stored)
- 1 million log lines/day â‰ˆ 1â€“1.5 GB/day in ES
- With 50Gi per node (150Gi total), ~30 days retention at moderate volume
- Metricbeat adds ~200MB/day for system + K8s metrics

---

## Backup & Retention Strategy

### The Professional Approach

**Never rely on PV/disk retention for backups.** The correct strategy is:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Elasticsearch     â”‚â”€â”€â”€â”€â–¶â”‚  Snapshot to S3       â”‚â”€â”€â”€â”€â–¶â”‚  Hetzner     â”‚
â”‚   (hot data on PV)  â”‚     â”‚  (daily, automated)   â”‚     â”‚  Object      â”‚
â”‚                     â”‚     â”‚                        â”‚     â”‚  Storage     â”‚
â”‚   ILM Policy:       â”‚     â”‚  SLM Policy:           â”‚     â”‚  (cold/warm) â”‚
â”‚   - Hot: 7 days     â”‚     â”‚  - Daily at 02:30      â”‚     â”‚              â”‚
â”‚   - Delete: 30 days â”‚     â”‚  - Retain 30 days      â”‚     â”‚  Cost: ~â‚¬5/  â”‚
â”‚                     â”‚     â”‚  - Min 5 snapshots     â”‚     â”‚  TB/month    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer 1: Index Lifecycle Management (ILM) â€” Automatic Cleanup

Filebeat and Metricbeat are configured with ILM. This controls **how long data stays in Elasticsearch**:

| Phase | Age | Action |
|-------|-----|--------|
| **Hot** | 0â€“7 days | Primary shard, full indexing, all queries served |
| **Warm** | 7â€“14 days | Read-only, force-merge to 1 segment (less disk) |
| **Delete** | 30 days | Index permanently deleted from ES |

To customize, go to **Kibana â†’ Stack Management â†’ Index Lifecycle Policies**.

### Layer 2: Elasticsearch Snapshots â€” Disaster Recovery

Register a snapshot repository (S3-compatible â€” Hetzner Object Storage):

```json
PUT _snapshot/s3_backup
{
  "type": "s3",
  "settings": {
    "bucket": "elk-backups",
    "endpoint": "fsn1.your-objectstorage.com",
    "protocol": "https"
  }
}
```

Create an automated Snapshot Lifecycle Management (SLM) policy:

```json
PUT _slm/policy/daily-snapshots
{
  "schedule": "0 30 2 * * ?",
  "name": "<daily-snap-{now/d}>",
  "repository": "s3_backup",
  "config": {
    "indices": ["filebeat-*", "metricbeat-*", "logs-*"],
    "ignore_unavailable": true
  },
  "retention": {
    "expire_after": "30d",
    "min_count": 5,
    "max_count": 50
  }
}
```

### Layer 3: Hetzner Volume Snapshots (Optional)

Hetzner supports server/volume snapshots at the cloud level. This is a brute-force backup but costs more:

```bash
# Create a snapshot of a Hetzner volume via API
hcloud volume create-snapshot <volume-id> --description "elk-backup-$(date +%F)"
```

> Not recommended as primary strategy â€” use ES snapshots instead.

---

## Post-Deployment Steps

### 1. Get Elasticsearch Password

ECK auto-generates the `elastic` user password as a Kubernetes secret:

```bash
kubectl get secret elk-es-elastic-user -n elastic-system \
  -o jsonpath='{.data.elastic}' | base64 -d ; echo
```

### 2. Access Kibana

After DNS is configured for `kibana.tests.software`:

```
URL:      https://kibana.tests.software
User:     elastic
Password: (from step 1)
```

### 3. Verify Data is Flowing

In Kibana:
- **Filebeat:** Discover â†’ Create data view for `filebeat-*`
- **Metricbeat:** Discover â†’ Create data view for `metricbeat-*`
- **Dashboards:** Metricbeat auto-loads Kibana dashboards (system, kubernetes)

### 4. Monitor Cluster Health

```bash
# Port-forward to ES
kubectl port-forward svc/elk-es-http -n elastic-system 9200:9200

# Check cluster health
curl -k -u "elastic:$(kubectl get secret elk-es-elastic-user \
  -n elastic-system -o jsonpath='{.data.elastic}' | base64 -d)" \
  https://localhost:9200/_cluster/health?pretty

# Check indices
curl -k -u "elastic:..." https://localhost:9200/_cat/indices?v

# Check storage usage
curl -k -u "elastic:..." https://localhost:9200/_cat/allocation?v
```

### 5. Verify Volumes

```bash
# Check PVCs created by ES StatefulSet
kubectl get pvc -n elastic-system

# Expected output:
# elasticsearch-data-elk-es-default-0   Bound   pvc-xxx   50Gi   hcloud-volumes
# elasticsearch-data-elk-es-default-1   Bound   pvc-yyy   50Gi   hcloud-volumes
# elasticsearch-data-elk-es-default-2   Bound   pvc-zzz   50Gi   hcloud-volumes

# Check PVs and their reclaim policy
kubectl get pv | grep elasticsearch

# Check Hetzner volumes (from local machine)
hcloud volume list
```

---

## Resource Summary

| Component | CPU Request | CPU Limit | Memory Request | Memory Limit | Storage |
|-----------|-------------|-----------|----------------|--------------|---------|
| ECK Operator | 100m | 500m | 256Mi | 512Mi | â€” |
| Elasticsearch (Ã—3) | 500m | 2000m | 2Gi | 4Gi | 50Gi each |
| Kibana | 250m | 1000m | 512Mi | 1Gi | â€” |
| Filebeat (per node) | 50m | 200m | 128Mi | 256Mi | â€” |
| Metricbeat (per node) | 50m | 200m | 128Mi | 256Mi | â€” |

**Total ELK node resource usage** (3 ES + 1 Kibana + 1 Operator + 1 Filebeat + 1 Metricbeat):
- CPU: ~2.1 cores request / ~7.9 cores limit
- Memory: ~7.5Gi request / ~14.3Gi limit
- Storage: 150Gi (3 Ã— 50Gi Hetzner Block Volumes)

> ğŸ’¡ **This is why `cx42` (4 vCPU / 16GB RAM) is the recommended minimum ELK node server type.**

---

## Troubleshooting

| Issue | Command | Solution |
|-------|---------|----------|
| ES pod stuck Pending | `kubectl describe pod elk-es-default-0 -n elastic-system` | Check nodeSelector, tolerations, or PVC binding |
| PVC stuck Pending | `kubectl describe pvc <name> -n elastic-system` | Check CSI driver is running, hcloud token valid |
| Cluster status RED | `curl .../_cluster/health?pretty` | Check unassigned shards: `_cat/shards?v&h=index,shard,prirep,state,unassigned.reason` |
| Filebeat not shipping | `kubectl logs ds/filebeat-beat-filebeat -n elastic-system` | Check ES connectivity, RBAC |
| Metricbeat no data | `kubectl logs ds/metricbeat-beat-metricbeat -n elastic-system` | Check kube-state-metrics service reachable |
| Volume full | `_cat/allocation?v` | Increase storage in elasticsearch.yaml or add ILM delete policy |
